# neurallanguage-notes
Simple notes and comments on papers about neural language learning from arxiv, ACL, EMNLP, NAACLE, etc. Originally, this is inspired by [Denny Britz's notes](https://github.com/dennybritz/deeplearning-papernotes).

#### TODO
- Pointing the Unknown Words
- Attend, Infer, Repeat Fast Scene Understanding with Generative Models
- How NOT To Evaluate Your Dialogue System An Empirical Study of
- Revisiting Semi-Supervised Learning with Graph Embeddings
- Neural Summarization by Extracting Sentences and Words
- Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints
- LSTM-BASED DEEP LEARNING MODELS FOR NONFACTOID
- Generating Visual Explanations
- A Compositional Approach to Language Modeling [[arxiv](http://arxiv.org/abs/1604.00100)]
- AttSum: Joint Learning of Focusing and Summarization with Neural Attention [[arxiv](http://arxiv.org/abs/1604.00125)]
- Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems [[arxiv](http://arxiv.org/abs/1511.06931)]
- The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations [[arxiv](http://arxiv.org/abs/1511.02301)]
- Building Machines That Learn and Think Like People [[arxiv](Building Machines That Learn and Think Like People)]
- A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories [[arxiv](https://arxiv.org/abs/1604.01696)]
- Revisiting Summarization Evaluation for Scientific Articles [[arxiv](http://arxiv.org/abs/1604.00400)]
- Reasoning About Pragmatics with Neural Listeners and Speakers [[arxiv](http://arxiv.org/abs/1604.00562)]
- Character-Level Question Answering with Attention [[arxiv](http://arxiv.org/abs/1604.00727)]
- Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1604.00734)]
- Top-down Tree Long Short-Term Memory Networks [[arxiv](http://arxiv.org/abs/1511.00060)]
- Recurrent Neural Network Grammars [[arxiv](http://arxiv.org/abs/1602.07776)]
- Pointing the Unknown Words [[arxiv](http://arxiv.org/abs/1603.08148)]
- Neural Programmer: Inducing Latent Programs with Gradient Descent [[arxiv](http://scholar.google.com/scholar_url?url=https://research.google.com/pubs/archive/44927.pdf&hl=en&sa=X&scisig=AAGBfm2VedkF99f2i9IB7m_Ki5ELxJ-SCQ&nossl=1&oi=scholaralrt)]
- Adversarial Autoencoders []
- Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition
- Net2Net: Accelerating Learning via Knowledge Transfer
- Neural Programmer: Inducing Latent Programs with Gradient Descent
- A Neural Conversational Model
- Document embedding with paragraph vectors
- Neural Language Correction with Character-Based Attention [[arxiv](http://arxiv.org/abs/1603.09727)]
- Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1604.01178)]
- Automatic Annotation of Structured Facts in Images
- Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves [[arxiv](https://arxiv.org/abs/1604.02038)]
- Building Machines That Learn and Think Like People [[arxiv](http://arxiv.org/pdf/1604.00289.pdf)]
- LARGER-CONTEXT LANGUAGE MODELLING WITH RECURRENT NEURAL NETWORK [[arxiv](http://arxiv.org/pdf/1511.03729v2.pdf)]
- A Diversity-Promoting Objective Function for Neural Conversation Model [[arxiv](http://arxiv.org/pdf/1510.03055v2.pdf)]
- TOWARDS PRINCIPLED UNSUPERVISED LEARNING [[arxiv](http://arxiv.org/pdf/1511.06440v2.pdf)]
- Sentence-Level Grammatical Error Identification as Sequence-to-Sequence
  Correction [[arxiv](https://arxiv.org/abs/1604.04677)]
- Hierarchical Attention Networks for Document Classification [[arxiv](http://scholar.google.co.kr/scholar_url?url=http://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf&hl=ko&sa=X&scisig=AAGBfm3Yksk0QUL3dBcaokFPC3DOF8CFvg&nossl=1&oi=scholaralrt)]
- Visual Storytelling [[arxiv](https://arxiv.org/abs/1604.03968)]
- Using Sentence-Level LSTM Language Models for Script Inference [[arxiv](https://arxiv.org/abs/1604.02993)]
- ABCNN: Attention-Based Convolutional Neural Network for Modeling
  Sentence Pairs [[arxiv](https://arxiv.org/abs/1512.05193)]
- Character-Level Question Answering with Attention [[arxiv](https://arxiv.org/abs/1604.00727)]
- Abstractive Text Summarization Using Sequence-to-Sequence RNNs and
  Beyond [[arxiv](https://arxiv.org/abs/1602.06023)]
- Sentence Compression by Deletion with LSTMs [[link](http://research.google.com/pubs/pub43852.html)]
- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units [[arxiv](http://arxiv.org/abs/1504.00941)]
- DenseCap: Fully Convolutional Localization Networks for Dense Captioning [[arxiv](http://cs.stanford.edu/people/karpathy//densecap.pdf)]
- TRAINING NEURAL NETWORKS ON PARTITIONED TRAINING DATA [[paper]()]
- Nonextensive information theoretical machine [[arxiv](https://arxiv.org/abs/1604.06153)]
- Training Deep Nets with Sublinear Memory Cost [[arxiv](https://arxiv.org/abs/1604.06174)]
- What we write about when we write about causality: Features of causal statements across large-scale social discourse [[arxiv](https://arxiv.org/abs/1604.05781)]
- Dialog-based Language Learning [[arxiv](https://arxiv.org/abs/1604.06045)]
- Question Answering via Integer Programming over Semi-Structured
  Knowledge [[arxiv](https://arxiv.org/abs/1604.06076)]


#### 2016-03
- Colorful Image Colorization [[paper](http://arxiv.org/abs/1603.08511)]  [[code](https://github.com/richzhang/colorization)] [[note](/notes/Colorful-Image-Colorization.md)]
