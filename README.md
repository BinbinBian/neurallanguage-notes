# neural language notes
Simple notes and comments on papers about neural language learning from arxiv, ACL, EMNLP, NAACL, etc. Originally, this is inspired by [Denny Britz's notes](https://github.com/dennybritz/deeplearning-papernotes).

#### TODO

- odelling Interaction of Sentence Pair with coupled-LSTMs [[arsiv](https://arxiv.org/abs/1605.05573)]
- Recurrent Neural Network for Text Classification with Multi-Task
  Learning [[arxiv](https://arxiv.org/abs/1605.05101)]
- Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling [[arxiv](https://arxiv.org/abs/1605.05110)]
- Rationale-Augmented Convolutional Neural Networks for Text
  Classification [[arxiv](https://arxiv.org/abs/1605.04469)]
- Reducing the Model Order of Deep Neural Networks Using Information
  Theory [[arxiv](https://arxiv.org/abs/1605.04859)]
- Compressing Word Embeddings [[arxiv](https://arxiv.org/abs/1511.06397)]
- Joint Event Extraction via Recurrent Neural Networks [[paper](http://scholar.google.co.kr/scholar_url?url=http://www.cs.nyu.edu/~thien/pubs/jointEE.pdf&hl=en&sa=X&scisig=AAGBfm1wUfB2tSO_zEvLwf0tPHJ1igLihw&nossl=1&oi=scholaralrt)]
- Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model [[paper](http://scholar.google.co.kr/scholar_url?url=http://arxiv.org/pdf/1605.03835&hl=en&sa=X&scisig=AAGBfm0D7ymB1zzlOBj8mGIrF47mc-7j0g&nossl=1&oi=scholaralrt)]
- Joint Extraction of Events and Entities within a Document Context [[paper](http://scholar.google.co.kr/scholar_url?url=http://www.cs.cmu.edu/~bishan/papers/joint_event_naacl16.pdf&hl=en&sa=X&scisig=AAGBfm12l_rfjffmHbRAg6VW26V4nSvhtQ&nossl=1&oi=scholaralrt)]
- Natural Language Semantics and Computability [[arxiv](https://arxiv.org/abs/1605.04122)]
- Transfer Hashing with Privileged Information [[arxiv](https://arxiv.org/abs/1605.04034)]
- Natural Language Inference by Tree-Based Convolution and Heuristic
  Matching [[arxiv](https://arxiv.org/abs/1512.08422)]
- Generating Sentences from a Continuous Space [[arxiv](https://arxiv.org/abs/1511.06349)]
- Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic
  Representation Learning [[arxiv](https://arxiv.org/abs/1605.03832)]
- Learning the Curriculum with Bayesian Optimization for Task-Specific
  Word Representation Learning [[arxiv](https://arxiv.org/abs/1605.03852)]
- Vocabulary Manipulation for Neural Machine Translation [[arxiv](https://arxiv.org/abs/1605.03209)]
- Tweet2Vec: Character-Based Distributed Representations for Social Media [[arxiv](https://arxiv.org/abs/1605.03481)]
------------------

- Chained Predictions Using Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1605.02346)]
- Boltzmann meets Nash: Energy-efficient routing in optical networks under uncertainty [[arxiv](https://arxiv.org/abs/1605.01451)]
- Modeling Rich Contexts for Sentiment Classification with LSTM [[arxiv](https://arxiv.org/abs/1605.01478)]
- Incorporating Selectional Preferences in Multi-hop Relation Extraction [[naacl16](http://rajarshd.github.io/papers/naaclakbc2016.pdf)]
- Word Ordering Without Syntax [[arxiv](http://arxiv.org/pdf/1604.08633.pdf)]
- Compositional Sentence Representation from Character within Large Context Text [[arxiv](https://arxiv.org/abs/1605.00482)]
- Abstractive Sentence Summarization with Attentive Recurrent Neural Networks [[arxiv](http://harvardnlp.github.io/papers/naacl16_summary.pdf)]
- Mixed Incremental Cross-Entropy REINFORCE ICLR 2016 [[github](https://github.com/facebookresearch/MIXER)]
- A library for probabilistic modeling, inference, and criticism. Deep generative models, variational inference. Runs on TensorFlow. [[github](https://github.com/blei-lab/edward)]
- Towards Conceptual Compression [[arxiv](https://arxiv.org/abs/1604.08772)]]
- Teaching natural language to computers [[arxiv](https://arxiv.org/abs/1604.08781)]
- Pointing the Unknown Words
- Attend, Infer, Repeat Fast Scene Understanding with Generative Models
- How NOT To Evaluate Your Dialogue System An Empirical Study of
- Revisiting Semi-Supervised Learning with Graph Embeddings
- Neural Summarization by Extracting Sentences and Words
- Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints
- LSTM-BASED DEEP LEARNING MODELS FOR NONFACTOID
- Generating Visual Explanations
- A Compositional Approach to Language Modeling [[arxiv](http://arxiv.org/abs/1604.00100)]
- AttSum: Joint Learning of Focusing and Summarization with Neural Attention [[arxiv](http://arxiv.org/abs/1604.00125)]
- Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems [[arxiv](http://arxiv.org/abs/1511.06931)]
- The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations [[arxiv](http://arxiv.org/abs/1511.02301)]
- Building Machines That Learn and Think Like People [[arxiv](Building Machines That Learn and Think Like People)]
- A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories [[arxiv](https://arxiv.org/abs/1604.01696)]
- Revisiting Summarization Evaluation for Scientific Articles [[arxiv](http://arxiv.org/abs/1604.00400)]
- Reasoning About Pragmatics with Neural Listeners and Speakers [[arxiv](http://arxiv.org/abs/1604.00562)]
- Character-Level Question Answering with Attention [[arxiv](http://arxiv.org/abs/1604.00727)]
- Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1604.00734)]
- Top-down Tree Long Short-Term Memory Networks [[arxiv](http://arxiv.org/abs/1511.00060)]
- Recurrent Neural Network Grammars [[arxiv](http://arxiv.org/abs/1602.07776)]
- Pointing the Unknown Words [[arxiv](http://arxiv.org/abs/1603.08148)]
- Neural Programmer: Inducing Latent Programs with Gradient Descent [[arxiv](http://scholar.google.com/scholar_url?url=https://research.google.com/pubs/archive/44927.pdf&hl=en&sa=X&scisig=AAGBfm2VedkF99f2i9IB7m_Ki5ELxJ-SCQ&nossl=1&oi=scholaralrt)]
- Adversarial Autoencoders []
- Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition
- Net2Net: Accelerating Learning via Knowledge Transfer
- Neural Programmer: Inducing Latent Programs with Gradient Descent
- A Neural Conversational Model
- Document embedding with paragraph vectors
- Neural Language Correction with Character-Based Attention [[arxiv](http://arxiv.org/abs/1603.09727)]
- Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1604.01178)]
- Automatic Annotation of Structured Facts in Images
- Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves [[arxiv](https://arxiv.org/abs/1604.02038)]
- Building Machines That Learn and Think Like People [[arxiv](http://arxiv.org/pdf/1604.00289.pdf)]
- LARGER-CONTEXT LANGUAGE MODELLING WITH RECURRENT NEURAL NETWORK [[arxiv](http://arxiv.org/pdf/1511.03729v2.pdf)]
- A Diversity-Promoting Objective Function for Neural Conversation Model [[arxiv](http://arxiv.org/pdf/1510.03055v2.pdf)]
- TOWARDS PRINCIPLED UNSUPERVISED LEARNING [[arxiv](http://arxiv.org/pdf/1511.06440v2.pdf)]
- Sentence-Level Grammatical Error Identification as Sequence-to-Sequence
  Correction [[arxiv](https://arxiv.org/abs/1604.04677)]
- Hierarchical Attention Networks for Document Classification [[arxiv](http://scholar.google.co.kr/scholar_url?url=http://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf&hl=ko&sa=X&scisig=AAGBfm3Yksk0QUL3dBcaokFPC3DOF8CFvg&nossl=1&oi=scholaralrt)]
- Visual Storytelling [[arxiv](https://arxiv.org/abs/1604.03968)]
- Using Sentence-Level LSTM Language Models for Script Inference [[arxiv](https://arxiv.org/abs/1604.02993)]
- ABCNN: Attention-Based Convolutional Neural Network for Modeling
  Sentence Pairs [[arxiv](https://arxiv.org/abs/1512.05193)]
- Character-Level Question Answering with Attention [[arxiv](https://arxiv.org/abs/1604.00727)]
- Abstractive Text Summarization Using Sequence-to-Sequence RNNs and
  Beyond [[arxiv](https://arxiv.org/abs/1602.06023)]
- Sentence Compression by Deletion with LSTMs [[link](http://research.google.com/pubs/pub43852.html)]
- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units [[arxiv](http://arxiv.org/abs/1504.00941)]
- DenseCap: Fully Convolutional Localization Networks for Dense Captioning [[arxiv](http://cs.stanford.edu/people/karpathy//densecap.pdf)]
- TRAINING NEURAL NETWORKS ON PARTITIONED TRAINING DATA [[paper]()]
- Nonextensive information theoretical machine [[arxiv](https://arxiv.org/abs/1604.06153)]
- Training Deep Nets with Sublinear Memory Cost [[arxiv](https://arxiv.org/abs/1604.06174)]
- What we write about when we write about causality: Features of causal statements across large-scale social discourse [[arxiv](https://arxiv.org/abs/1604.05781)]
- Dialog-based Language Learning [[arxiv](https://arxiv.org/abs/1604.06045)]
- Question Answering via Integer Programming over Semi-Structured
  Knowledge [[arxiv](https://arxiv.org/abs/1604.06076)]
- Dialog-based Language Learning [[arxiv](http://arxiv.org/pdf/1604.06045.pdf)]
- Bridging LSTM Architecture and the Neural Dynamics during Reading [[arxiv](https://arxiv.org/abs/1604.06635)]
- Neural Generative Question Answering [[arxiv](https://arxiv.org/abs/1512.01337)]
- Recurrent Memory Networks for Language Modeling [[arxiv](https://arxiv.org/abs/1601.01272)]
- Colorful Image Colorization [[paper](http://arxiv.org/abs/1603.08511)]  [[code](https://github.com/richzhang/colorization)] [[note](/notes/Colorful-Image-Colorization.md)]
- ...
